# -*- coding: utf-8 -*-
"""Applied Ml mini project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UuNCEnE5dajj9U4AI76x_GYNphLVLul5

![image.png](data:image/png;base64,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)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import math

import random
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

"""# 1- Dataset 1"""

from google.colab import drive
drive.mount('/content/drive')



"""# Loading Dataset

texte en italique
"""

df2=pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/Applied ML Mini project 1/Dataset/Qualitative_Bankruptcy.data.txt')

#log in rahma
df=pd.read_excel(r'/content/drive/MyDrive/Colab Notebooks/Applied ML Mini project 1/Dataset/ENB2012_data.xlsx')

#login bita
df=pd.read_excel(r'/content/drive/MyDrive/ENB2012_data.xlsx')

df.head(20)

"""The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.

Specifically:

X1 Relative Compactness, X2 Surface Area,
X3 Wall Area,
X4 Roof Area,
X5 Overall Height,
X6 Orientation,
X7 Glazing Area,
X8 Glazing Area Distribution,
y1 Heating Load,
y2 Cooling Load.
"""

df.columns = ["Relative Compactness","Surface Area","Wall Area",
                "Roof Area", "Overall Height","Orientation","Glazing Area",
                "Glazing Area Distribution", "Heating Load", "Cooling Load"]

df.head(20)

"""
# 1-1 Exploring data"""

df.shape

"""This dataset houses 768 observations and 10 features. It has 8 independant variables used to predict 2 dependant variables."""

df.info()

df.isna().sum()

"""According to the previous 2 commands, the data has only integer and float variables and it does not hold any missing values.

# 1-2 Data overall statistics
"""

df.describe()

"""



*  We can conclude that there are no outliers because there is no significant difference between the 75th quantile and the maximum value for any of the features.


*   The range of values of the features is not the same for that we choose to normalize the date in order to make it in the same scale.


"""

for col in df.columns:
    print("{c} Column has {u} unique values".format(c=col,u = np.count_nonzero(df[col].unique())))

"""Analysing target variables"""

##Counting unique elements in Cooling Load column
np.count_nonzero(df['Heating Load'].unique())

##Counting unique elements in Heating Load column
np.count_nonzero(df['Cooling Load'].unique())



"""Target variables are numerical."""

sns.pairplot(df)

round(df.corr(),3)

plt.figure(figsize=(20,10))
sns.heatmap(round(df.corr(),3), annot=True)

"""

*   We have "Orientation" and "Glazing Area distribution" that are poorly correlated to the target variables so we suggest **to drop them.**
*   The highest positive correlation is between target variables and Overall height. We notice the highest negative correlation of target variables is with Roof Area.

*  We noticed also a high correlation between certain independant variables:
Relative Compactness with surface Area, Roof Area and Overall Height.
Surface Area with Roof Area and Overall height ( We could drop surface area or Relative Compactness as it they are  highly correlated and the two other features are highly correlated to target variable hence they are more important to be kept for prediction)( I choose relative compactness because less correlated than surface area to the target variables)

I would say we drop "Orientation"  "Glazing Area distribution" and " Relative Compactness"




"""

## data distribution

plt.figure(figsize=(35,36))

l = df.columns.values
number_of_columns=10
number_of_rows = len(l)-1/number_of_columns
for i in range(0,len(l)):
    plt.subplot(number_of_rows ,number_of_columns,i+1)
    sns.histplot(df[l[i]],kde=True) 
plt.show()

"""

> The features have an overall uniform distribution with a combination of normal distribution for the target variables.

"""

plt.figure(figsize=(15,6))

boxplot = df.boxplot(column= ["Surface Area","Wall Area",
                "Roof Area", "Overall Height","Glazing Area", "Heating Load", "Cooling Load"] )
boxplot.plot()
plt.show()

"""There are no outliers in this dataset."""

df.describe()

"""> By looking at the previous experiments, we could notice that for example the mean of surface area is around 3000 times the mean of glazing area, hence the gap between the scale of the different feature. For that we need to perform normalization on the data in order to put all the values on the same scale.

#1-3 Data Cleaning and model fitting

Dropping using SelectKbest

We started by selecting 5 features at the beginning by dropping only surface Area however, it yielded bad performance. Therefore, we opted to drop another feature with a high correlation with the target variables which is Roof Area.
"""

from sklearn.feature_selection import SelectKBest,  f_regression

X=df.iloc[:,0:8]
Y=df.iloc[:,9]

Y

select = SelectKBest(score_func=f_regression,k=6)
z = select.fit_transform(X,Y)

cols_idxs = select.get_support(indices=True)

features_df_new = X.iloc[:,cols_idxs]
features_df_new.columns

features=features_df_new.drop(columns=['Surface Area','Roof Area'])
features.columns

columns=[ 'Relative Compactness', 'Wall Area',
       'Overall Height', 'Glazing Area']
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_new= pd.DataFrame(scaler.fit_transform(features), columns=features.columns)

df_new.describe()

from sklearn.model_selection import train_test_split
target=['Heating Load','Cooling Load']
X=df_new[columns].copy()
y=df[target]
X_train, X_test,y_train, y_test = train_test_split(X,y ,
                                   random_state=2, 
                                   test_size=0.2, 
                                   shuffle=True)

X_train

class LinearRegression:
  
    def __init__(self, add_bias=True):
        self.add_bias = add_bias
      
        pass
    
    def fit(self, x, y):
        if x.ndim == 1:
            x = x[:, None]                         #add a dimension for the features
        N = x.shape[0]
      
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])    #add bias by adding a constant feature of value 1
        self.w = np.linalg.lstsq(x, y)[0] 
  
        
                #return w for the least square difference
      

        return self
    
    
    def predict(self, x):
        N=x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])
        yh = x@self.w                             #predict the y values
        return yh

model_CF = LinearRegression()

y_pred=model_CF.fit(X_train,y_train).predict(X_test)

y_test

model_CF.w

def r2_score(y_real,y_pred):
    corr_coef = np.corrcoef(y_real,y_pred)[0][1]
    r2 = corr_coef**2
    return r2
def MAE(y_real,y_pred):
    m = sum(abs(y_real-y_pred))/len(y_pred)
    return m
def MSE(y_real,y_pred):

    m = np.square(np.subtract(y_real,y_pred)).mean()
    return m

y_test.iloc[:,0]

"""Performance on Test:"""

##Heating Load
mse=MSE(y_test.iloc[:,0].values,y_pred[:,0])
R2=r2_score(y_test.iloc[:,0],y_pred[:,0])
mae=MAE(y_test.iloc[:,0].values,y_pred[:,0])
RMSE = math.sqrt(mse)
print("Root Mean Square Error:\n",RMSE)
print('Mean Squared Error is', mse)
print('r2 score is', R2)
print('Mean Absolute Error:\n',mae)

##Cooling Load
mse=MSE(y_test.iloc[:,1].values,y_pred[:,1])
R2=r2_score(y_test.iloc[:,1].values,y_pred[:,1])
RMSE =math.sqrt(mse)
mae=MAE(y_test.iloc[:,1].values,y_pred[:,1])

print('Mean Squared Error is', mse)
print('r2 score is', R2)
print("Root Mean Square Error:\n",RMSE)
print('Mean Absolute Error:\n',mae)

"""Performance on Train:"""

##Heating Load
y_pred_train=model_CF.fit(X_train,y_train).predict(X_train)
mse=MSE(y_train.iloc[:,0],y_pred_train[:,0])
R2=r2_score(y_train.iloc[:,0],y_pred_train[:,0])
RMSE=math.sqrt(mse)
mae=MAE(y_train.iloc[:,0].values,y_pred_train[:,0])

print('Mean Squared Error is', mse)
print('r2 score is', R2)
print("Root Mean Square Error:\n",RMSE)
print('Mean Absolute Error:\n',mae)

##Cooling Load
y_pred=model_CF.fit(X_train,y_train).predict(X_train)
mse=MSE(y_train.iloc[:,1],y_pred[:,1])
R2=r2_score(y_train.iloc[:,1],y_pred[:,1])
RMSE=math.sqrt(mse)
mae=MAE(y_train.iloc[:,1].values,y_pred[:,1])

print('Mean Squared Error is', mse)
print('r2 score is', R2)
print('Mean Absolute Error:\n',mae)

print("Root Mean Square Error:\n",RMSE)

model_CF.w

"""#1-4 mini bach gradient descent linear reg"""

D=X_train.shape[1]
X_train

import time

def predict( x,w,bias=True):
        yh = np.dot(x,w)                            #predict the y values
        return yh


def gradient(X, y, w):
	N = X.shape[0]
	
	yh = np.dot(X, w)
	grad =np.dot(X.T, (yh - y))
	return grad

# function to compute the error for current values of weights


def cost_fun(X, y, w):
	N=X.shape[0]
	yh = np.dot(X, w)
	J = (1/(2*N))* np.sum( (y-yh)**2)
	
	return J

def create_mini_batches(X, y, batch_size):
	mini_batches = []
	data = np.hstack((X, y))
 
	np.random.shuffle(data);
	n_minibatches = data.shape[0] // batch_size
	j = 0
	for j in range(n_minibatches + 1):
		mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
		X_mini = mini_batch[:, :-1]		
		Y_mini = mini_batch[:, -1].reshape((-1, 1))
		mini_batches.append((X_mini, Y_mini))
	if data.shape[0] % batch_size != 0:
		mini_batch = data[j * batch_size:data.shape[0]]
		X_mini = mini_batch[:, :-1]
		Y_mini = mini_batch[:, -1].reshape((-1, 1))
	 
		mini_batches.append((X_mini, Y_mini))
	return mini_batches

def Mini_batch_SGD(X_train,y_train,X_test,y_test, lr=0.01,batch_size=8,bias=True):
  
  if X_train.ndim == 1:

    X_train = X_train[:,None]
    X_test = X_test[:,None]

  N = X_train.shape[0]
  N0= X_test.shape[0]
  if bias:
      X_train = np.column_stack([X_train,np.ones(N)])
      X_test = np.column_stack([X_test,np.ones(N0)])
  w = np.zeros((X_train.shape[1],1))
  cost_list=[]
  cost_err=[]
  epochs=100
  for epoch in range(epochs):
      mini_batches = create_mini_batches(X_train, y_train, batch_size)
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        w = w - lr * gradient(X_mini, y_mini, w)
        cost_list.append(cost_fun(X_mini, y_mini, w))
        cost_err.append((cost_fun(X_test,y_test,w)))

  y_pred = predict(X_test,w)
    
    
  return w , cost_list, cost_err,y_pred

class Mini_batch_GD:

  def __init__(self,lr=0.001,batch_size=8,max_iters=100,add_bias=True):
        self.lr = lr
        self.max_iters = max_iters
        self.batch_size = batch_size
        self.cost_list=[]
        self.cost_err=[]
        self.cost_list_epoch=[]
        self.cost_err_epoch=[]
     
        self.add_bias=add_bias

  def run2 ( self,gradient,x,y,x_test,y_test,w):
    w = np.zeros((x.shape[1],1))
    if x_test.ndim == 1:
            x_test = x_test[:, None]
    if self.add_bias:
            N = x_test.shape[0]
            x_test = np.column_stack([x_test,np.ones(N)])

    for epoch in range(self.max_iters):
      mini_batches = create_mini_batches(x, y, self.batch_size)
      for mini_batch in mini_batches:

        X_mini, y_mini = mini_batch
        w = w - self.lr * gradient(X_mini, y_mini, w)
        self.cost_list.append(cost_fun(X_mini, y_mini, w))
        self.cost_err.append((cost_fun(x_test,y_test,w)))
      self.cost_list_epoch.append((cost_fun(X_mini,y_mini,w)))
      self.cost_err_epoch.append((cost_fun(x_test,y_test,w)))
      

    return w

  def run (self, gradient,x,y,w):
    for epoch in range(self.max_iters):
      mini_batches = create_mini_batches(x, y, self.batch_size)
      for mini_batch in mini_batches:

        X_mini, y_mini = mini_batch
        w = w - self.lr * gradient(X_mini, y_mini, w)
    return w

  def get_cost_train(self):
    return self.cost_list
  def get_cost_test(self):
    return self.cost_err
  def get_cost_train_epoch(self):
    return self.cost_list_epoch
  def get_cost_test_epoch(self):
    return self.cost_err_epoch

class LinearRegression:

    def __init__(self,add_bias=True):
        self.add_bias = add_bias
        #self.max_iter=max_iter
        #self.batch_size=batch_size
        #self.lr=lr
        pass
            
    def fit_plot(self, x, y,x_test,y_test,optimizer):
        

        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        def cost_fun1(X, y, w):
          N=X.shape[0]
          yh = np.dot(X, w)
          J = (1/(2*N))* np.sum( (y-yh)**2)
          return J
        def cost_fun(X,y,w):
          N=X.shape[0]
          yh = np.dot(X, w)


          J=(1/2)*np.square(np.subtract(y,yh)).mean()
          return J
        def gradient(X, y, w):
          N = X.shape[0]
          yh = np.dot(X, w)
          grad =np.dot(X.T, (yh - y))
          return grad
        def create_mini_batches(X, y, batch_size):
          mini_batches = []
          data = np.hstack((X, y))
          np.random.shuffle(data)
          n_minibatches = data.shape[0] // batch_size
          j = 0
          for j in range(n_minibatches + 1):
            mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
            X_mini = mini_batch[:, :-1]
            Y_mini = mini_batch[:, -1].reshape((-1, 1))
            mini_batches.append((X_mini, Y_mini))
          if data.shape[0] % batch_size != 0:	
              mini_batch = data[j * batch_size:data.shape[0]]	
              X_mini = mini_batch[:, :-1]
              Y_mini = mini_batch[:, -1].reshape((-1, 1))
              mini_batches.append((X_mini, Y_mini))
          return mini_batches
        w=np.zeros(D)
        self.w = optimizer.run2(gradient, x, y,x_test,y_test, w)      # run the optimizer to get the optimal weights
        return self



    def fit(self, x, y):
        

        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        def cost_fun(X, y, w):
          N=X.shape[0]
          yh = np.dot(X, w)
          J = (1/(2*N))* np.sum( (y-yh)**2)
          return J

        def gradient(X, y, w):
          N = X.shape[0]
          yh = np.dot(X, w)
          grad =np.dot(X.T, (yh - y))
          return grad
        def create_mini_batches(X, y, batch_size):
          mini_batches = []
          data = np.hstack((X, y))
          np.random.shuffle(data)
          n_minibatches = data.shape[0] // batch_size
          j = 0
          for j in range(n_minibatches + 1):
            mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
            X_mini = mini_batch[:, :-1]
            Y_mini = mini_batch[:, -1].reshape((-1, 1))
            mini_batches.append((X_mini, Y_mini))
          if data.shape[0] % batch_size != 0:	
              mini_batch = data[j * batch_size:data.shape[0]]	
              X_mini = mini_batch[:, :-1]
              Y_mini = mini_batch[:, -1].reshape((-1, 1))
              mini_batches.append((X_mini, Y_mini))
          return mini_batches

        def opti_run ( gradient,x,y,w):
          for epoch in range(100):
            mini_batches = create_mini_batches(x, y, 64)
            for mini_batch in mini_batches:
              X_mini, y_mini = mini_batch
              w = w - 0.01 * gradient(X_mini, y_mini, w)
          return w
        
        w = np.zeros(D)
        self.w = opti_run(gradient, x, y, w)      # run the optimizer to get the optimal weights
        
        return self
    
    def predict(self, x):
        N=x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])
        yh = x@self.w
        return yh

##First target: Heating Load

"""Batch size 32 """

optimizer = Mini_batch_GD(lr=0.01,batch_size=32, max_iters=100)
model = LinearRegression()
y_pred=model.fit_plot(X_train,y_train.iloc[:,:1].values,X_test,y_test.iloc[:,:1].values,optimizer).predict(X_test)

train_err=optimizer.get_cost_train()
test_err=optimizer.get_cost_test()

plt.plot(train_err)
plt.plot(test_err)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

def r2_score(y_real,y_pred):
    corr_coef = np.corrcoef(y_real,y_pred)[0][1]
    r2 = corr_coef**2
    return r2
    
def MSE(y_real,y_pred):

    m = np.square(np.subtract(y_real,y_pred)).mean()
    return m

y_pred1=pd.DataFrame(y_pred,columns=['Cooling Load'])

r2=r2_score(y_test.iloc[:,0],y_pred1.iloc[:,0])
r2

metric=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)
metric

train_err_epoch_32=optimizer.get_cost_train_epoch()
test_err_epoch_32=optimizer.get_cost_test_epoch()

plt.plot(train_err_epoch_32)
plt.plot(test_err_epoch_32)

labels=['Train Error','Test error']
plt.xlabel("Number of epochs")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

"""Batch size 8"""

optimizer = Mini_batch_GD(lr=0.0001,batch_size=8, max_iters=100)
model = LinearRegression()
y_pred=model.fit_plot(X_train,y_train.iloc[:,:1].values,X_test,y_test.iloc[:,:1].values,optimizer).predict(X_test)

y_pred1=pd.DataFrame(y_pred,columns=['Cooling Load'])

metric=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)
metric

train_err=optimizer.get_cost_train()
test_err=optimizer.get_cost_test()

plt.plot(train_err)
plt.plot(test_err)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

train_err_epoch_8=optimizer.get_cost_train_epoch()
test_err_epoch_8=optimizer.get_cost_test_epoch()

plt.plot(train_err_epoch_8)
plt.plot(test_err_epoch_8)
labels=['Train Error','Test error']
plt.xlabel("Number of epochs")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

"""Batch size 16 """

optimizer = Mini_batch_GD(lr=0.001,batch_size=16, max_iters=100)
model = LinearRegression()
y_pred=model.fit_plot(X_train,y_train.iloc[:,:1].values,X_test,y_test.iloc[:,:1].values,optimizer).predict(X_test)

y_pred1=pd.DataFrame(y_pred,columns=['Cooling Load'])

metric=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)
metric

train_err=optimizer.get_cost_train()
test_err=optimizer.get_cost_test()

plt.plot(train_err)
plt.plot(test_err)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

train_err_epoch_16=optimizer.get_cost_train_epoch()
test_err_epoch_16=optimizer.get_cost_test_epoch()

plt.plot(train_err_epoch_16)
plt.plot(test_err_epoch_16)

labels=['Train Error','Test error']
plt.xlabel("Number of epochs")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

"""Batch size 64 """

optimizer = Mini_batch_GD(lr=0.01,batch_size=64, max_iters=100)
model = LinearRegression()
y_pred=model.fit_plot(X_train,y_train.iloc[:,:1].values,X_test,y_test.iloc[:,:1].values,optimizer).predict(X_test)

y_pred1=pd.DataFrame(y_pred,columns=['Cooling Load'])

metric=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)
metric

train_err=optimizer.get_cost_train()
test_err=optimizer.get_cost_test()

plt.plot(train_err)
plt.plot(test_err)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

train_err_epoch_64=optimizer.get_cost_train_epoch()
test_err_epoch_64=optimizer.get_cost_test_epoch()

plt.plot(train_err_epoch_64)
plt.plot(test_err_epoch_64)
labels=['Train Error','Test error']
plt.xlabel("Number of epochs")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

plt.plot(train_err_epoch_32, label='Train Error 32 batch size lr=0.01')
plt.plot(test_err_epoch_32, label='Test Error 32 batch size lr=0.01')
plt.plot(train_err_epoch_8, label='Train Error 8 batchsize lr=0.0001')
plt.plot(test_err_epoch_8, label = 'Test Error 8 batch size lr= 0.0001')
plt.xlabel('Number of Epochs')
plt.ylabel('MSE')
plt.title('Comparison between two batch sizes')
plt.legend()


plt.show()

"""batch size 128"""

optimizer = Mini_batch_GD(lr=0.01,batch_size=128, max_iters=100)
model = LinearRegression()
y_pred=model.fit_plot(X_train,y_train.iloc[:,:1].values,X_test,y_test.iloc[:,:1].values,optimizer).predict(X_test)



y_pred1=pd.DataFrame(y_pred,columns=['Cooling Load'])

metric=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)
metric

train_err=optimizer.get_cost_train()
test_err=optimizer.get_cost_test()

plt.plot(train_err)
plt.plot(test_err)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

##With batch size 128, it explodes.

##Second Target : Cooling Load

optimizer = Mini_batch_GD(lr=0.01,batch_size=64, max_iters=100)
model = LinearRegression()
y_pred_second_target=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)

train_err2=optimizer.get_cost_train()
test_err2=optimizer.get_cost_test()

plt.plot(train_err2)
plt.plot(test_err2)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

train_err_epoch2=optimizer.get_cost_train_epoch()
test_err_epoch2=optimizer.get_cost_test_epoch()

plt.plot(train_err_epoch2)
plt.plot(test_err_epoch2)
labels=['Train Error','Test error']
plt.xlabel("Number of epochs")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

MB_weights_2target=model.w
MB_weights_2target

model_CF.w[:,1]

y_pred2=pd.DataFrame(y_pred_second_target,columns=['Cooling Load'])

r2=r2_score(y_test.iloc[:,1],y_pred2.iloc[:,0])
r2

metric=MSE(y_test.iloc[:,1].values,y_pred2.iloc[:,0].values)
metric

"""Batch size 32"""

optimizer = Mini_batch_GD(lr=0.01,batch_size=32, max_iters=100)
model = LinearRegression()
y_pred_second_target=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)

train_err2=optimizer.get_cost_train()
test_err2=optimizer.get_cost_test()

plt.plot(train_err2)
plt.plot(test_err2)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

"""Batch size 16"""

optimizer = Mini_batch_GD(lr=0.001,batch_size=16, max_iters=100)
model = LinearRegression()
y_pred_second_target=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)

train_err2=optimizer.get_cost_train()
test_err2=optimizer.get_cost_test()

plt.plot(train_err2)
plt.plot(test_err2)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

y_pred2=pd.DataFrame(y_pred_second_target,columns=['Cooling Load'])

r2=r2_score(y_test.iloc[:,1],y_pred2.iloc[:,0])
r2

metric=MSE(y_test.iloc[:,1].values,y_pred2.iloc[:,0].values)
metric

"""Batch size 8 """

optimizer = Mini_batch_GD(lr=0.001,batch_size=8, max_iters=100)
model = LinearRegression()
y_pred_second_target=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)

train_err2=optimizer.get_cost_train()
test_err2=optimizer.get_cost_test()

plt.plot(train_err2)
plt.plot(test_err2)
labels=['Train Error','Test error']
plt.xlabel("Number of iterations")
plt.ylabel("Cost")
#plt.ylim(0,10)
plt.legend(labels)
plt.show()

y_pred2=pd.DataFrame(y_pred_second_target,columns=['Cooling Load'])

r2=r2_score(y_test.iloc[:,1],y_pred2.iloc[:,0])
r2

metric=MSE(y_test.iloc[:,1].values,y_pred2.iloc[:,0].values)
metric

##Different batch_sizes

for batch_size in [8,16,32,64,128]:
  optimizer = Mini_batch_GD(lr=0.01,batch_size=batch_size, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  y_pred1=pd.DataFrame(y_pred,columns=['Heating Load'])
  r2=r2_score(y_test.iloc[:,0],y_pred1.iloc[:,0])
  mse=MSE(y_test.iloc[:,0].values,y_pred1.iloc[:,0].values)

  wei=model.w
  weight_results = "Weights of the model for the first target for mini_batch {k} are  {weights} ".format(k=batch_size, weights = wei )
  r2score_results = "R2 score of the model for the first target for mini_batch {k} is  {score} ".format(k=batch_size, score = r2 )
  MSE_results = "MSE of the model for the first target for mini_batch {k} is  {mse} ".format(k=batch_size, mse = mse )


  print(weight_results)
  print(r2score_results)
  print(MSE_results)

for batch_size in [8,16,32,64,128]:
  optimizer = Mini_batch_GD(lr=0.01,batch_size=batch_size, max_iters=100)
  model = LinearRegression()
  y_pred_sec=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)
  y_pred2=pd.DataFrame(y_pred_sec,columns=['Cooling Load'])
  r2=r2_score(y_test.iloc[:,0],y_pred2.iloc[:,0])
  mse=MSE(y_test.iloc[:,0],y_pred2.iloc[:,0].values)

  wei=model.w
  weight_results = "Weights of the model for the second target for mini_batch {k} are  {weights} ".format(k=batch_size, weights = wei )
  r2score_results = "R2 score of the model for the second target for mini_batch {k} is  {score} ".format(k=batch_size, score = r2 )
  MSE_results = "MSE of the model for the second target for mini_batch {k} is  {mse} ".format(k=batch_size, mse = mse )


  print(weight_results)
  print(r2score_results)
  print(MSE_results)

#Batch_size=32 is the best let's try varying its learning rate for the Heating Load

##With lr =0.1 , it explodes

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=16, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 16')
plt.show()

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=64, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 64')
plt.show()

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=32, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 32')
plt.show()

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=8, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 8')
plt.show()

##Second target Cooling Load

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=16, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 16 (Cooling load)')
plt.show()

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=32, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 32 (Cooling Load')
plt.show()

for lr in [0.01,0.001,0.0001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=64, max_iters=100)
  model = LinearRegression()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'lr ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 64 (Cooling Load)')
plt.show()

##Learning Curves

def Mini_batch_SGD2(X_train,y_train,X_test,y_test, lr=0.01,batch_size=8,bias=True):
  
  if X_train.ndim == 1:

    X_train = X_train[:,None]
    X_test = X_test[:,None]

  N = X_train.shape[0]
  N0= X_test.shape[0]
  if bias:
      X_train = np.column_stack([X_train,np.ones(N)])
      X_test = np.column_stack([X_test,np.ones(N0)])
  w = np.zeros((X_train.shape[1],1))
 
  epochs=100
  for epoch in range(epochs):
      mini_batches = create_mini_batches(X_train, y_train, batch_size)
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        w = w - lr * gradient(X_mini, y_mini, w)
  train=cost_fun(X_mini, y_mini, w)
  test=cost_fun(X_test,y_test,w)
    
    
  return train, test[0]

def sample_size(k):
  target=['Cooling Load','Heating Load']
  X=df_new[columns].copy()
  y=df[target]

  X_train, X_test,y_train, y_test = train_test_split(X,y ,
                                   random_state=5, 
                                   train_size=k, 
                                   shuffle=True)
  
  train_err,test_err=Mini_batch_SGD2(X_train,y_train.iloc[:,1:],X_test,y_test.iloc[:,1:], lr=0.01,batch_size=32,bias=True)
  return train_err,test_err

l=[]
l2=[]
label=['Train error','Test Error']
x_axes=['20%','30%','40%','60%','70%','80%']

for k in [0.2,0.3,0.4,0.6,0.7,0.8]:

  train_err,test_err=sample_size(k)
  l.append(train_err)
  l2.append(test_err)
plt.plot(x_axes,l)
plt.plot(x_axes,l2)

plt.legend(labels)
plt.xlabel('Training size percentage')
plt.ylabel('MSE')
plt.title('Learning curve')
plt.show()



"""## 2- Dataset 2"""

# Mohamad Login directory 
data = "/content/drive/MyDrive/Qualitative_Bankruptcy.data.txt"
file1 = open(data, "r")

datapoint = file1.read()
print(datapoint)

#Mohamad Login 
import pandas as pd
df2=pd.read_csv(r'/content/drive/MyDrive/Qualitative_Bankruptcy.data.txt',dtype="category")

df2.shape

#add column names
df2.columns = [ "Industrial_Risk"
     ,"Management_Risk"
     ,"Financial_Flexibility"
     ,"Credibility"
     ,"Competitiveness"
     ,"Operating_Risk","Class"]

df2

df2.info()

df2.shape

"""turn to binary with one hot encoding"""

#df2_Risk = pd.get_dummies(df2.iloc[:,1])

# from sklearn.preprocessing import LabelEncoder
# from sklearn.preprocessing import OneHotEncoder
# from numpy import argmax

"""**2-1 Data cleaning**

Coverting the caetgorical dataset into dummy varibles
"""

df3=df2. copy()
#df3['Industrial_Risk'] = labelencoder.fit_transform(df3['Industrial_Risk'])
#df3['Management_Risk'] = labelencoder.fit_transform(df3['Management_Risk'])
#df3['Financial_Flexibility'] = labelencoder.fit_transform(df3['Financial_Flexibility'])
#df3['Credibility'] = labelencoder.fit_transform(df3['Credibility'])
#df3['Competitiveness'] = labelencoder.fit_transform(df3['Competitiveness'])
#df3['Operating_Risk'] = labelencoder.fit_transform(df3['Operating_Risk'])

df3 = pd.get_dummies(df3, columns=['Industrial_Risk', 'Management_Risk', 'Financial_Flexibility', 'Credibility', 'Competitiveness', 'Operating_Risk'])

df3['Bankrupt'] = (df3['Class'] == 'B').apply(np.uint8)
df3.drop('Class', axis=1, inplace=True)

df3.info()

"""**2-2- Logistic model**"""

X2=df3.iloc[:,0:18]
X2

#y2_s having the data as series, y2_df dataframe version of the same dataset
y2_s=df3.iloc[:,18]
y2_df = y2_s.to_frame(name="Bankrupt")

#spliting the dataset into traing and test set 80/20
from sklearn.model_selection import train_test_split
X_train, X_test,y_train, y_test = train_test_split(X2,y2_df ,
                                   random_state=104, 
                                   test_size=0.2, 
                                   shuffle=True)
  
X_test.shape
y_test.shape

"""Define Gradient Descent algorithm """

#Logistic function
logistic = lambda z: 1./ (1 + np.exp(-z))

#Cost function
def cost_fn(x, y, w):
    N, D = x.shape                                                       
    z = np.dot(x, w)
    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))  #log1p calculates log(1+x) to remove floating point inaccuracies 
    return J

def gradient(self, x, y):
    N,D = x.shape
    yh = logistic(np.dot(x, self.w))    # predictions  size N
    grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points
    return grad                         # size D

"""The Gradient Descent method for solving the Logistic Regression """

class GradientDescent:
    
    def __init__(self, learning_rate=.01, max_iters=1e4, epsilon=1e-8, record_history=False):
        self.learning_rate = learning_rate
        self.max_iters = max_iters
        self.record_history = record_history
        self.epsilon = epsilon
        if record_history:
            self.w_history = []                 #to store the weight history for visualization
            
    def run(self, gradient_fn, x, y, w):
        grad = np.inf
        t = 1
        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:
            grad = gradient_fn(x, y, w)               # compute the gradient with present weight
            w = w - self.learning_rate * grad         # weight update step
            if self.record_history:
                self.w_history.append(w)
            t += 1
        return w

"""Wrting a class funtion for logistic regression and define the weight to recall

Define the Logisitic Regression
"""

class LogisticRegression:
    
    def __init__(self, add_bias=True, learning_rate=.01, epsilon=1e-4, max_iters=1e5, verbose=False):
        self.add_bias = add_bias
        self.learning_rate = learning_rate
        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients 
        self.max_iters = max_iters                    #maximum number of iteration of gradient descent
        self.verbose = verbose
        
    def fit(self, x, y):
        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        self.w = np.zeros(D)
        g = np.inf 
        t = 0
        # the code snippet below is for gradient descent
        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:
            g = self.gradient(x, y)
            self.w = self.w - self.learning_rate * g 
            t += 1
        
        if self.verbose:
            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')
            print(f'the weight found: {self.w}')
        return self
    def fit_plot(self, x, y,x_test,y_test,optimizer):
        

        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        def cost_fun1(X, y, w):
          N=X.shape[0]
          yh = logistic(np.dot(x, self.w))  
          J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))  #log1p calculates log(1+x) to remove floating point inaccuracies 
          return J
        def cost_fun(X,y,w):
          N=X.shape[0]
          yh = np.dot(X, w)


          J=(1/2)*np.square(np.subtract(y,yh)).mean()
          return J
    def weight(self, x, y):
        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        self.w = np.zeros(D)
        g = np.inf 
        t = 0
        # the code snippet below is for gradient descent
        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:
            g = self.gradient(x, y)
            self.w = self.w - self.learning_rate * g 
            t += 1
        
        if self.verbose:
            print(f'the weight found: {self.w}')
        return self.w
    
    def predict(self, x):
        if x.ndim == 1:
            x = x[:, None]
        Nt = x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(Nt)])
        yh = logistic(np.dot(x,self.w))            #predict output
        return yh

LogisticRegression.gradient = gradient            #initialize the gradient method of the LogisticRegression class with gradient function

"""The 4 learning rate for Gradient Descent Logistic regression"""

#learning rate (0.001, 0.01, 0.05, 0.1)
model1 = LogisticRegression(learning_rate = 0.001)
model2= LogisticRegression(learning_rate = 0.01)
model3 = LogisticRegression(learning_rate = 0.05)
model4 = LogisticRegression(learning_rate = 0.1)

y_train1 =y_train.squeeze()
y_pred1=model1.fit(X_train,y_train1).predict(X_test)
y_pred2=model2.fit(X_train,y_train1).predict(X_test)
y_pred3=model3.fit(X_train,y_train1).predict(X_test)
y_pred4=model4.fit(X_train,y_train1).predict(X_test)

#learning curve for diffrent training size 
from mlxtend.plotting import plot_learning_curves

# Loading some example data
y_test1=y_test.squeeze()


plot_learning_curves(X_train, y_train1, X_test, y_test1, model2,scoring = 'log_loss')
plot_learning_curves(X_train, y_train1, X_test, y_test1, model1,scoring = 'log_loss')
plot_learning_curves(X_train, y_train1, X_test, y_test1, model3,scoring = 'log_loss')
plot_learning_curves(X_train, y_train1, X_test, y_test1, model4,scoring = 'log_loss')
plt.show()

#metric for evluation of regression
def r2_score(y_real,y_pred):
    corr_coef = np.corrcoef(y_real,y_pred)[0][1]
    r2 = corr_coef**2
    return r2

def MSE(y_real,y_pred):
    m = np.square(np.subtract(y_real,y_pred)).mean()
    return m

def Loss(y, y_hat):
    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))
    return loss

def accuracy(y_pred,y_test):
    return np.mean(y_pred==y_test)

y_test1=y_test.squeeze()

# weight for the best model logistic regression based on accuarcy (learning rate = 0.05)
weights= model3.weight(X_train,y_train1)

weights

#learning rate equal to 0.001
y_test1=y_test.squeeze()
metric=MSE(y_test1,y_pred1)
R2=r2_score(y_test1,y_pred1)
loss= Loss(y_test1,y_pred1)
print('Loss function value',loss)
print('Mean Absolute Error is', metric)
print('r2 score is', R2)

#learning rate equal to 0.01
y_test1=y_test.squeeze()
metric=MSE(y_test1,y_pred2)
R2=r2_score(y_test1,y_pred2)
loss= Loss(y_test1,y_pred2)
print('Loss function value',loss)
print('Mean Absolute Error is', metric)
print('r2 score is', R2)

#learning rate equal to 0.05
y_test1=y_test.squeeze()
metric=MSE(y_test1,y_pred3)
R2=r2_score(y_test1,y_pred3)
loss= Loss(y_test1,y_pred3)
print('Loss function value',loss)
print('Mean Absolute Error is', metric)
print('r2 score is', R2)

#learning rate equal to 0.1
y_test1=y_test.squeeze()
metric=MSE(y_test1,y_pred4)
R2=r2_score(y_test1,y_pred4)
loss= Loss(y_test1,y_pred4)
print('Loss function value',loss)
print('Mean Absolute Error is', metric)
print('r2 score is', R2)

from mlxtend.plotting import plot_learning_curves


#learning curve for the best model with learning rates of 0.05 log loss function


plot_learning_curves(X_train, y_train1, X_test, y_test1, model3,scoring = 'log_loss')
plt.show()

from mlxtend.plotting import plot_learning_curves


#learning curve for the best model with learning rates of 0.05, area onther the ROC-AUC is 0.998 

plot_learning_curves(X_train, y_train1, X_test, y_test1, model3,scoring = 'roc_auc')
plt.show()

y_train.iloc[:,:1]

#confusion matrix for learning rates of (0.001, 0.01, 0.05, 0.1) to evaulte the model performance 
from sklearn import metrics 
y_pred_1=np.round_(y_pred1)
y_pred_2=np.round_(y_pred2)
y_pred_3=np.round_(y_pred3)
y_pred_4=np.round_(y_pred4)


cnf_matrix1 = metrics.confusion_matrix(y_test1, y_pred_1)
cnf_matrix2 = metrics.confusion_matrix(y_test1, y_pred_2)
cnf_matrix3 = metrics.confusion_matrix(y_test1, y_pred_3)
cnf_matrix4 = metrics.confusion_matrix(y_test1, y_pred_4)

cnf_matrix1

#learning rate 0.001
from sklearn.metrics import classification_report
print(classification_report(y_test1, y_pred_1))

#learning rate 0.01
from sklearn.metrics import classification_report
print(classification_report(y_test1, y_pred_2))

#learning rate 0.05
from sklearn.metrics import classification_report
print(classification_report(y_test1, y_pred_3))

#learning rate 0.1
from sklearn.metrics import classification_report
print(classification_report(y_test1, y_pred_4))

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test1, y_pred3)

# this shape one of the way to show the performance of the model with area under curve highest value would be 1.00 
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve
def plot_roc_curve(Y_test, model_probs):

  # calculate AUC
  model_auc = roc_auc_score(Y_test, model_probs)
  # summarize score
  print('Model: ROC AUC=%.3f' % (model_auc))

fpr, tpr, threshold = metrics.roc_curve(y_test1, y_pred1)

roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""**2-2 Mini Batch SGD**"""

import time


# function to compute gradient of error function w.r.t. weights


def gradient(X, y, w):
	yh = logistic(np.dot(X, w))
	grad = np.dot(X.transpose(), (yh - y))
	return grad

# function to compute the error for current values of weights


def cost_fn(x, y, w):
    N, D = x.shape                                                       
    z = np.dot(x, w)
    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))  #log1p calculates log(1+x) to remove floating point inaccuracies 
    return J

# function to create a list containing several mini-batches


def create_mini_batches(X, y, batch_size):
	mini_batches = []
	data = np.hstack((X, y))
	np.random.shuffle(data)
	n_minibatches = data.shape[0] // batch_size
	j = 0

	for j in range(n_minibatches + 1):
		mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
		X_mini = mini_batch[:, :-1]
		Y_mini = mini_batch[:, -1].reshape((-1, 1))
		mini_batches.append((X_mini, Y_mini))
	if data.shape[0] % batch_size != 0:
		mini_batch = data[j * batch_size:data.shape[0]]
		X_mini = mini_batch[:, :-1]
		Y_mini = mini_batch[:, -1].reshape((-1, 1))
		mini_batches.append((X_mini, Y_mini))
	return mini_batches

def Mini_batch_SGD(X_train,y_train,X_test,y_test, learning_rate=0.01,batch_size=8,bias=True):
  
  if X_train.ndim == 1:

    X_train = X_train[:,None]
    X_test = X_test[:,None]

  N = X_train.shape[0]
  N0= X_test.shape[0]
  if bias:
      X_train = np.column_stack([X_train,np.ones(N)])
      X_test = np.column_stack([X_test,np.ones(N0)])
  w = np.zeros((X_train.shape[1],1))
  cost_list=[]
  cost_err=[]
  epochs=100
  for epoch in range(epochs):
      mini_batches = create_mini_batches(X_train, y_train, batch_size)
      for mini_batch in mini_batches:
        X_mini, y_mini = mini_batch
        w = w - learning_rate * gradient(X_mini, y_mini, w)
        cost_list.append(cost_fn(X_mini, y_mini, w))
        cost_err.append((cost_fn(X_test,y_test,w)))
    
    
  return w , cost_list, cost_err

X_test.head()

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=128,bias=True)

plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

test_err[-1]

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=64,bias=True)

plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=32,bias=True)

plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=16,bias=True)

plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=8,bias=True)
time.time()
plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

w , cost_list1,test_err = Mini_batch_SGD (X_train , y_train.iloc[:,:1], X_test, y_test.iloc[:,:1], learning_rate=0.05, batch_size=4,bias=True)

plt.plot(cost_list1)
labels=['Train Error','Test Error']
plt.plot(test_err)
plt.xlabel("Number of iterations")
plt.ylabel("Cost Function")
plt.legend(labels)
plt.show()

def sigmoid(x,w,b):
    return 1/(1+np.exp(-(np.dot(x,w)+b)))
def loss(x,w,y,b):
    s=sigmoid(x,w,b)
    return np.mean(-(y*np.log(s))- ((1-y)*np.log(1-s)))
def grad(x,y,w,b):
    s=sigmoid(x,w,b)    
    return np.dot(x.T,(s-y))/x.shape[0]

class LogisticRegression2:

    def __init__(self,add_bias=True):
        self.add_bias = add_bias
        #self.max_iter=max_iter
        #self.batch_size=batch_size
        #self.lr=lr
        pass
            
    def fit_plot(self, x, y,x_test,y_test,optimizer):
        

        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        def cost_fun1(X, y, w):
          N=X.shape[0]
          z = np.dot(x, w)
          J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z))) 
          return J
        def cost_fun(X,y,w):
          N=X.shape[0]
          z = np.dot(x, w)
          J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z))) 
          return J

        def create_mini_batches(X, y, batch_size):
          mini_batches = []
          data = np.hstack((X, y))
          np.random.shuffle(data)
          n_minibatches = data.shape[0] // batch_size
          j = 0
          for j in range(n_minibatches + 1):
            mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
            X_mini = mini_batch[:, :-1]
            Y_mini = mini_batch[:, -1].reshape((-1, 1))
            mini_batches.append((X_mini, Y_mini))
          if data.shape[0] % batch_size != 0:	
              mini_batch = data[j * batch_size:data.shape[0]]	
              X_mini = mini_batch[:, :-1]
              Y_mini = mini_batch[:, -1].reshape((-1, 1))
              mini_batches.append((X_mini, Y_mini))
          return mini_batches
        w=np.zeros(D)
        self.w = optimizer.run2(gradient, x, y,x_test,y_test, w)      # run the optimizer to get the optimal weights
        return self



    def fit(self, x, y):
        

        if x.ndim == 1:
            x = x[:, None]
        if self.add_bias:
            N = x.shape[0]
            x = np.column_stack([x,np.ones(N)])
        N,D = x.shape
        def cost_fun(X, y, w):
          N=X.shape[0]
          z = np.dot(x, w)
          J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z))) 
          return J

        def create_mini_batches(X, y, batch_size):
          mini_batches = []
          data = np.hstack((X, y))
          np.random.shuffle(data)
          n_minibatches = data.shape[0] // batch_size
          j = 0
          for j in range(n_minibatches + 1):
            mini_batch = data[j * batch_size:(j + 1)*batch_size, :]
            X_mini = mini_batch[:, :-1]
            Y_mini = mini_batch[:, -1].reshape((-1, 1))
            mini_batches.append((X_mini, Y_mini))
          if data.shape[0] % batch_size != 0:	
              mini_batch = data[j * batch_size:data.shape[0]]	
              X_mini = mini_batch[:, :-1]
              Y_mini = mini_batch[:, -1].reshape((-1, 1))
              mini_batches.append((X_mini, Y_mini))
          return mini_batches

        def opti_run ( gradient,x,y,w):
          for epoch in range(100):
            mini_batches = create_mini_batches(x, y, 64)
            for mini_batch in mini_batches:
              X_mini, y_mini = mini_batch
              w = w - 0.01 * gradient(X_mini, y_mini, w)
          return w
        
        w = np.zeros(D)
        self.w = opti_run(gradient, x, y, w)      # run the optimizer to get the optimal weights
        
        return self
    
    def predict(self, x):
        N=x.shape[0]
        if self.add_bias:
            x = np.column_stack([x,np.ones(N)])
        yh = x@self.w
        return yh

for lr in [0.01,0.001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=32, max_iters=100)
  model = LogisticRegression2()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'Eps ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 32')
plt.show()

for lr in [0.01,0.001]:
  optimizer = Mini_batch_GD(lr=lr,batch_size=64, max_iters=100)
  model = LogisticRegression2()
  y_pred=model.fit_plot(X_train,y_train.iloc[:,:1],X_test,y_test.iloc[:,:1],optimizer).predict(X_test)
  train_err=optimizer.get_cost_train()
  plt.plot(train_err, label= 'Eps ='+ str(lr))
  plt.xlabel("Number of iteration")
  plt.ylabel("Training Error")
  plt.legend()
  plt.title('Training curves for different learning rates with batch size 64')
plt.show()